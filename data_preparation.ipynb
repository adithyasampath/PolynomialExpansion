{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def create_dataset(root_folder=\"./dataset\", seed=1234, debug=False):\n",
    "    data_path = os.path.join(root_folder, \"dataset.txt\")\n",
    "    train_path, val_path, test_path = os.path.join(root_folder, \"train.txt\"), \\\n",
    "                                        os.path.join(root_folder, \"val.txt\"), \\\n",
    "                                        os.path.join(root_folder, \"test.txt\")\n",
    "    data = np.loadtxt(data_path, dtype=str, delimiter=\"=\")\n",
    "    train, val_test = train_test_split(data, test_size = 0.2, random_state=seed)\n",
    "    val, test = train_test_split(val_test, test_size = 0.5, random_state=seed)\n",
    "    if debug:\n",
    "        print(train.shape, val.shape, test.shape)\n",
    "    np.savetxt(train_path, train, fmt=\"%s\", delimiter='=')\n",
    "    np.savetxt(val_path, val, fmt=\"%s\", delimiter='=')\n",
    "    np.savetxt(test_path, test, fmt=\"%s\", delimiter='=')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800000, 2) (100000, 2) (100000, 2)\n"
     ]
    }
   ],
   "source": [
    "create_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (encoder): Encoder(\n",
      "    (tok_embedding): Embedding(36, 256)\n",
      "    (pos_embedding): Embedding(100, 256)\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): MultiHeadAttentionLayer(\n",
      "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
      "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
      "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): EncoderLayer(\n",
      "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): MultiHeadAttentionLayer(\n",
      "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
      "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
      "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): EncoderLayer(\n",
      "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): MultiHeadAttentionLayer(\n",
      "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
      "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
      "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (tok_embedding): Embedding(36, 256)\n",
      "    (pos_embedding): Embedding(100, 256)\n",
      "    (layers): ModuleList(\n",
      "      (0): DecoderLayer(\n",
      "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): MultiHeadAttentionLayer(\n",
      "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (encoder_attention): MultiHeadAttentionLayer(\n",
      "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
      "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
      "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): DecoderLayer(\n",
      "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): MultiHeadAttentionLayer(\n",
      "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (encoder_attention): MultiHeadAttentionLayer(\n",
      "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
      "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
      "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): DecoderLayer(\n",
      "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): MultiHeadAttentionLayer(\n",
      "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (encoder_attention): MultiHeadAttentionLayer(\n",
      "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
      "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
      "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (fc_out): Linear(in_features=256, out_features=36, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "model = torch.load(\"models/transformer/nlayers3hdim256nhead10/best_model_full_epoch45.pth\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of model parameters: 4,032,548\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Number of model parameters: {count_parameters(model):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================================================================================\n",
      "                                                   Kernel Shape  \\\n",
      "Layer                                                             \n",
      "0_encoder.Embedding_tok_embedding                     [256, 36]   \n",
      "1_encoder.Embedding_pos_embedding                    [256, 100]   \n",
      "2_encoder.Dropout_dropout                                     -   \n",
      "3_encoder.layers.0.self_attention.Linear_fc_q        [256, 256]   \n",
      "4_encoder.layers.0.self_attention.Linear_fc_k        [256, 256]   \n",
      "5_encoder.layers.0.self_attention.Linear_fc_v        [256, 256]   \n",
      "6_encoder.layers.0.self_attention.Dropout_dropout             -   \n",
      "7_encoder.layers.0.self_attention.Linear_fc_o        [256, 256]   \n",
      "8_encoder.layers.0.Dropout_dropout                            -   \n",
      "9_encoder.layers.0.LayerNorm_self_attn_layer_norm         [256]   \n",
      "10_encoder.layers.0.positionwise_feedforward.Li...   [256, 512]   \n",
      "11_encoder.layers.0.positionwise_feedforward.Dr...            -   \n",
      "12_encoder.layers.0.positionwise_feedforward.Li...   [512, 256]   \n",
      "13_encoder.layers.0.Dropout_dropout                           -   \n",
      "14_encoder.layers.0.LayerNorm_ff_layer_norm               [256]   \n",
      "15_encoder.layers.1.self_attention.Linear_fc_q       [256, 256]   \n",
      "16_encoder.layers.1.self_attention.Linear_fc_k       [256, 256]   \n",
      "17_encoder.layers.1.self_attention.Linear_fc_v       [256, 256]   \n",
      "18_encoder.layers.1.self_attention.Dropout_dropout            -   \n",
      "19_encoder.layers.1.self_attention.Linear_fc_o       [256, 256]   \n",
      "20_encoder.layers.1.Dropout_dropout                           -   \n",
      "21_encoder.layers.1.LayerNorm_self_attn_layer_norm        [256]   \n",
      "22_encoder.layers.1.positionwise_feedforward.Li...   [256, 512]   \n",
      "23_encoder.layers.1.positionwise_feedforward.Dr...            -   \n",
      "24_encoder.layers.1.positionwise_feedforward.Li...   [512, 256]   \n",
      "25_encoder.layers.1.Dropout_dropout                           -   \n",
      "26_encoder.layers.1.LayerNorm_ff_layer_norm               [256]   \n",
      "27_encoder.layers.2.self_attention.Linear_fc_q       [256, 256]   \n",
      "28_encoder.layers.2.self_attention.Linear_fc_k       [256, 256]   \n",
      "29_encoder.layers.2.self_attention.Linear_fc_v       [256, 256]   \n",
      "30_encoder.layers.2.self_attention.Dropout_dropout            -   \n",
      "31_encoder.layers.2.self_attention.Linear_fc_o       [256, 256]   \n",
      "32_encoder.layers.2.Dropout_dropout                           -   \n",
      "33_encoder.layers.2.LayerNorm_self_attn_layer_norm        [256]   \n",
      "34_encoder.layers.2.positionwise_feedforward.Li...   [256, 512]   \n",
      "35_encoder.layers.2.positionwise_feedforward.Dr...            -   \n",
      "36_encoder.layers.2.positionwise_feedforward.Li...   [512, 256]   \n",
      "37_encoder.layers.2.Dropout_dropout                           -   \n",
      "38_encoder.layers.2.LayerNorm_ff_layer_norm               [256]   \n",
      "39_decoder.Embedding_tok_embedding                    [256, 36]   \n",
      "40_decoder.Embedding_pos_embedding                   [256, 100]   \n",
      "41_decoder.Dropout_dropout                                    -   \n",
      "42_decoder.layers.0.self_attention.Linear_fc_q       [256, 256]   \n",
      "43_decoder.layers.0.self_attention.Linear_fc_k       [256, 256]   \n",
      "44_decoder.layers.0.self_attention.Linear_fc_v       [256, 256]   \n",
      "45_decoder.layers.0.self_attention.Dropout_dropout            -   \n",
      "46_decoder.layers.0.self_attention.Linear_fc_o       [256, 256]   \n",
      "47_decoder.layers.0.Dropout_dropout                           -   \n",
      "48_decoder.layers.0.LayerNorm_self_attn_layer_norm        [256]   \n",
      "49_decoder.layers.0.encoder_attention.Linear_fc_q    [256, 256]   \n",
      "50_decoder.layers.0.encoder_attention.Linear_fc_k    [256, 256]   \n",
      "51_decoder.layers.0.encoder_attention.Linear_fc_v    [256, 256]   \n",
      "52_decoder.layers.0.encoder_attention.Dropout_d...            -   \n",
      "53_decoder.layers.0.encoder_attention.Linear_fc_o    [256, 256]   \n",
      "54_decoder.layers.0.Dropout_dropout                           -   \n",
      "55_decoder.layers.0.LayerNorm_enc_attn_layer_norm         [256]   \n",
      "56_decoder.layers.0.positionwise_feedforward.Li...   [256, 512]   \n",
      "57_decoder.layers.0.positionwise_feedforward.Dr...            -   \n",
      "58_decoder.layers.0.positionwise_feedforward.Li...   [512, 256]   \n",
      "59_decoder.layers.0.Dropout_dropout                           -   \n",
      "60_decoder.layers.0.LayerNorm_ff_layer_norm               [256]   \n",
      "61_decoder.layers.1.self_attention.Linear_fc_q       [256, 256]   \n",
      "62_decoder.layers.1.self_attention.Linear_fc_k       [256, 256]   \n",
      "63_decoder.layers.1.self_attention.Linear_fc_v       [256, 256]   \n",
      "64_decoder.layers.1.self_attention.Dropout_dropout            -   \n",
      "65_decoder.layers.1.self_attention.Linear_fc_o       [256, 256]   \n",
      "66_decoder.layers.1.Dropout_dropout                           -   \n",
      "67_decoder.layers.1.LayerNorm_self_attn_layer_norm        [256]   \n",
      "68_decoder.layers.1.encoder_attention.Linear_fc_q    [256, 256]   \n",
      "69_decoder.layers.1.encoder_attention.Linear_fc_k    [256, 256]   \n",
      "70_decoder.layers.1.encoder_attention.Linear_fc_v    [256, 256]   \n",
      "71_decoder.layers.1.encoder_attention.Dropout_d...            -   \n",
      "72_decoder.layers.1.encoder_attention.Linear_fc_o    [256, 256]   \n",
      "73_decoder.layers.1.Dropout_dropout                           -   \n",
      "74_decoder.layers.1.LayerNorm_enc_attn_layer_norm         [256]   \n",
      "75_decoder.layers.1.positionwise_feedforward.Li...   [256, 512]   \n",
      "76_decoder.layers.1.positionwise_feedforward.Dr...            -   \n",
      "77_decoder.layers.1.positionwise_feedforward.Li...   [512, 256]   \n",
      "78_decoder.layers.1.Dropout_dropout                           -   \n",
      "79_decoder.layers.1.LayerNorm_ff_layer_norm               [256]   \n",
      "80_decoder.layers.2.self_attention.Linear_fc_q       [256, 256]   \n",
      "81_decoder.layers.2.self_attention.Linear_fc_k       [256, 256]   \n",
      "82_decoder.layers.2.self_attention.Linear_fc_v       [256, 256]   \n",
      "83_decoder.layers.2.self_attention.Dropout_dropout            -   \n",
      "84_decoder.layers.2.self_attention.Linear_fc_o       [256, 256]   \n",
      "85_decoder.layers.2.Dropout_dropout                           -   \n",
      "86_decoder.layers.2.LayerNorm_self_attn_layer_norm        [256]   \n",
      "87_decoder.layers.2.encoder_attention.Linear_fc_q    [256, 256]   \n",
      "88_decoder.layers.2.encoder_attention.Linear_fc_k    [256, 256]   \n",
      "89_decoder.layers.2.encoder_attention.Linear_fc_v    [256, 256]   \n",
      "90_decoder.layers.2.encoder_attention.Dropout_d...            -   \n",
      "91_decoder.layers.2.encoder_attention.Linear_fc_o    [256, 256]   \n",
      "92_decoder.layers.2.Dropout_dropout                           -   \n",
      "93_decoder.layers.2.LayerNorm_enc_attn_layer_norm         [256]   \n",
      "94_decoder.layers.2.positionwise_feedforward.Li...   [256, 512]   \n",
      "95_decoder.layers.2.positionwise_feedforward.Dr...            -   \n",
      "96_decoder.layers.2.positionwise_feedforward.Li...   [512, 256]   \n",
      "97_decoder.layers.2.Dropout_dropout                           -   \n",
      "98_decoder.layers.2.LayerNorm_ff_layer_norm               [256]   \n",
      "99_decoder.Linear_fc_out                              [256, 36]   \n",
      "\n",
      "                                                      Output Shape    Params  \\\n",
      "Layer                                                                          \n",
      "0_encoder.Embedding_tok_embedding                     [1, 21, 256]    9.216k   \n",
      "1_encoder.Embedding_pos_embedding                     [1, 21, 256]     25.6k   \n",
      "2_encoder.Dropout_dropout                             [1, 21, 256]         -   \n",
      "3_encoder.layers.0.self_attention.Linear_fc_q         [1, 21, 256]   65.792k   \n",
      "4_encoder.layers.0.self_attention.Linear_fc_k         [1, 21, 256]   65.792k   \n",
      "5_encoder.layers.0.self_attention.Linear_fc_v         [1, 21, 256]   65.792k   \n",
      "6_encoder.layers.0.self_attention.Dropout_dropout   [1, 8, 21, 21]         -   \n",
      "7_encoder.layers.0.self_attention.Linear_fc_o         [1, 21, 256]   65.792k   \n",
      "8_encoder.layers.0.Dropout_dropout                    [1, 21, 256]         -   \n",
      "9_encoder.layers.0.LayerNorm_self_attn_layer_norm     [1, 21, 256]     512.0   \n",
      "10_encoder.layers.0.positionwise_feedforward.Li...    [1, 21, 512]  131.584k   \n",
      "11_encoder.layers.0.positionwise_feedforward.Dr...    [1, 21, 512]         -   \n",
      "12_encoder.layers.0.positionwise_feedforward.Li...    [1, 21, 256]  131.328k   \n",
      "13_encoder.layers.0.Dropout_dropout                   [1, 21, 256]         -   \n",
      "14_encoder.layers.0.LayerNorm_ff_layer_norm           [1, 21, 256]     512.0   \n",
      "15_encoder.layers.1.self_attention.Linear_fc_q        [1, 21, 256]   65.792k   \n",
      "16_encoder.layers.1.self_attention.Linear_fc_k        [1, 21, 256]   65.792k   \n",
      "17_encoder.layers.1.self_attention.Linear_fc_v        [1, 21, 256]   65.792k   \n",
      "18_encoder.layers.1.self_attention.Dropout_dropout  [1, 8, 21, 21]         -   \n",
      "19_encoder.layers.1.self_attention.Linear_fc_o        [1, 21, 256]   65.792k   \n",
      "20_encoder.layers.1.Dropout_dropout                   [1, 21, 256]         -   \n",
      "21_encoder.layers.1.LayerNorm_self_attn_layer_norm    [1, 21, 256]     512.0   \n",
      "22_encoder.layers.1.positionwise_feedforward.Li...    [1, 21, 512]  131.584k   \n",
      "23_encoder.layers.1.positionwise_feedforward.Dr...    [1, 21, 512]         -   \n",
      "24_encoder.layers.1.positionwise_feedforward.Li...    [1, 21, 256]  131.328k   \n",
      "25_encoder.layers.1.Dropout_dropout                   [1, 21, 256]         -   \n",
      "26_encoder.layers.1.LayerNorm_ff_layer_norm           [1, 21, 256]     512.0   \n",
      "27_encoder.layers.2.self_attention.Linear_fc_q        [1, 21, 256]   65.792k   \n",
      "28_encoder.layers.2.self_attention.Linear_fc_k        [1, 21, 256]   65.792k   \n",
      "29_encoder.layers.2.self_attention.Linear_fc_v        [1, 21, 256]   65.792k   \n",
      "30_encoder.layers.2.self_attention.Dropout_dropout  [1, 8, 21, 21]         -   \n",
      "31_encoder.layers.2.self_attention.Linear_fc_o        [1, 21, 256]   65.792k   \n",
      "32_encoder.layers.2.Dropout_dropout                   [1, 21, 256]         -   \n",
      "33_encoder.layers.2.LayerNorm_self_attn_layer_norm    [1, 21, 256]     512.0   \n",
      "34_encoder.layers.2.positionwise_feedforward.Li...    [1, 21, 512]  131.584k   \n",
      "35_encoder.layers.2.positionwise_feedforward.Dr...    [1, 21, 512]         -   \n",
      "36_encoder.layers.2.positionwise_feedforward.Li...    [1, 21, 256]  131.328k   \n",
      "37_encoder.layers.2.Dropout_dropout                   [1, 21, 256]         -   \n",
      "38_encoder.layers.2.LayerNorm_ff_layer_norm           [1, 21, 256]     512.0   \n",
      "39_decoder.Embedding_tok_embedding                    [1, 21, 256]    9.216k   \n",
      "40_decoder.Embedding_pos_embedding                    [1, 21, 256]     25.6k   \n",
      "41_decoder.Dropout_dropout                            [1, 21, 256]         -   \n",
      "42_decoder.layers.0.self_attention.Linear_fc_q        [1, 21, 256]   65.792k   \n",
      "43_decoder.layers.0.self_attention.Linear_fc_k        [1, 21, 256]   65.792k   \n",
      "44_decoder.layers.0.self_attention.Linear_fc_v        [1, 21, 256]   65.792k   \n",
      "45_decoder.layers.0.self_attention.Dropout_dropout  [1, 8, 21, 21]         -   \n",
      "46_decoder.layers.0.self_attention.Linear_fc_o        [1, 21, 256]   65.792k   \n",
      "47_decoder.layers.0.Dropout_dropout                   [1, 21, 256]         -   \n",
      "48_decoder.layers.0.LayerNorm_self_attn_layer_norm    [1, 21, 256]     512.0   \n",
      "49_decoder.layers.0.encoder_attention.Linear_fc_q     [1, 21, 256]   65.792k   \n",
      "50_decoder.layers.0.encoder_attention.Linear_fc_k     [1, 21, 256]   65.792k   \n",
      "51_decoder.layers.0.encoder_attention.Linear_fc_v     [1, 21, 256]   65.792k   \n",
      "52_decoder.layers.0.encoder_attention.Dropout_d...  [1, 8, 21, 21]         -   \n",
      "53_decoder.layers.0.encoder_attention.Linear_fc_o     [1, 21, 256]   65.792k   \n",
      "54_decoder.layers.0.Dropout_dropout                   [1, 21, 256]         -   \n",
      "55_decoder.layers.0.LayerNorm_enc_attn_layer_norm     [1, 21, 256]     512.0   \n",
      "56_decoder.layers.0.positionwise_feedforward.Li...    [1, 21, 512]  131.584k   \n",
      "57_decoder.layers.0.positionwise_feedforward.Dr...    [1, 21, 512]         -   \n",
      "58_decoder.layers.0.positionwise_feedforward.Li...    [1, 21, 256]  131.328k   \n",
      "59_decoder.layers.0.Dropout_dropout                   [1, 21, 256]         -   \n",
      "60_decoder.layers.0.LayerNorm_ff_layer_norm           [1, 21, 256]     512.0   \n",
      "61_decoder.layers.1.self_attention.Linear_fc_q        [1, 21, 256]   65.792k   \n",
      "62_decoder.layers.1.self_attention.Linear_fc_k        [1, 21, 256]   65.792k   \n",
      "63_decoder.layers.1.self_attention.Linear_fc_v        [1, 21, 256]   65.792k   \n",
      "64_decoder.layers.1.self_attention.Dropout_dropout  [1, 8, 21, 21]         -   \n",
      "65_decoder.layers.1.self_attention.Linear_fc_o        [1, 21, 256]   65.792k   \n",
      "66_decoder.layers.1.Dropout_dropout                   [1, 21, 256]         -   \n",
      "67_decoder.layers.1.LayerNorm_self_attn_layer_norm    [1, 21, 256]     512.0   \n",
      "68_decoder.layers.1.encoder_attention.Linear_fc_q     [1, 21, 256]   65.792k   \n",
      "69_decoder.layers.1.encoder_attention.Linear_fc_k     [1, 21, 256]   65.792k   \n",
      "70_decoder.layers.1.encoder_attention.Linear_fc_v     [1, 21, 256]   65.792k   \n",
      "71_decoder.layers.1.encoder_attention.Dropout_d...  [1, 8, 21, 21]         -   \n",
      "72_decoder.layers.1.encoder_attention.Linear_fc_o     [1, 21, 256]   65.792k   \n",
      "73_decoder.layers.1.Dropout_dropout                   [1, 21, 256]         -   \n",
      "74_decoder.layers.1.LayerNorm_enc_attn_layer_norm     [1, 21, 256]     512.0   \n",
      "75_decoder.layers.1.positionwise_feedforward.Li...    [1, 21, 512]  131.584k   \n",
      "76_decoder.layers.1.positionwise_feedforward.Dr...    [1, 21, 512]         -   \n",
      "77_decoder.layers.1.positionwise_feedforward.Li...    [1, 21, 256]  131.328k   \n",
      "78_decoder.layers.1.Dropout_dropout                   [1, 21, 256]         -   \n",
      "79_decoder.layers.1.LayerNorm_ff_layer_norm           [1, 21, 256]     512.0   \n",
      "80_decoder.layers.2.self_attention.Linear_fc_q        [1, 21, 256]   65.792k   \n",
      "81_decoder.layers.2.self_attention.Linear_fc_k        [1, 21, 256]   65.792k   \n",
      "82_decoder.layers.2.self_attention.Linear_fc_v        [1, 21, 256]   65.792k   \n",
      "83_decoder.layers.2.self_attention.Dropout_dropout  [1, 8, 21, 21]         -   \n",
      "84_decoder.layers.2.self_attention.Linear_fc_o        [1, 21, 256]   65.792k   \n",
      "85_decoder.layers.2.Dropout_dropout                   [1, 21, 256]         -   \n",
      "86_decoder.layers.2.LayerNorm_self_attn_layer_norm    [1, 21, 256]     512.0   \n",
      "87_decoder.layers.2.encoder_attention.Linear_fc_q     [1, 21, 256]   65.792k   \n",
      "88_decoder.layers.2.encoder_attention.Linear_fc_k     [1, 21, 256]   65.792k   \n",
      "89_decoder.layers.2.encoder_attention.Linear_fc_v     [1, 21, 256]   65.792k   \n",
      "90_decoder.layers.2.encoder_attention.Dropout_d...  [1, 8, 21, 21]         -   \n",
      "91_decoder.layers.2.encoder_attention.Linear_fc_o     [1, 21, 256]   65.792k   \n",
      "92_decoder.layers.2.Dropout_dropout                   [1, 21, 256]         -   \n",
      "93_decoder.layers.2.LayerNorm_enc_attn_layer_norm     [1, 21, 256]     512.0   \n",
      "94_decoder.layers.2.positionwise_feedforward.Li...    [1, 21, 512]  131.584k   \n",
      "95_decoder.layers.2.positionwise_feedforward.Dr...    [1, 21, 512]         -   \n",
      "96_decoder.layers.2.positionwise_feedforward.Li...    [1, 21, 256]  131.328k   \n",
      "97_decoder.layers.2.Dropout_dropout                   [1, 21, 256]         -   \n",
      "98_decoder.layers.2.LayerNorm_ff_layer_norm           [1, 21, 256]     512.0   \n",
      "99_decoder.Linear_fc_out                               [1, 21, 36]    9.252k   \n",
      "\n",
      "                                                   Mult-Adds  \n",
      "Layer                                                         \n",
      "0_encoder.Embedding_tok_embedding                     9.216k  \n",
      "1_encoder.Embedding_pos_embedding                      25.6k  \n",
      "2_encoder.Dropout_dropout                                  -  \n",
      "3_encoder.layers.0.self_attention.Linear_fc_q        65.536k  \n",
      "4_encoder.layers.0.self_attention.Linear_fc_k        65.536k  \n",
      "5_encoder.layers.0.self_attention.Linear_fc_v        65.536k  \n",
      "6_encoder.layers.0.self_attention.Dropout_dropout          -  \n",
      "7_encoder.layers.0.self_attention.Linear_fc_o        65.536k  \n",
      "8_encoder.layers.0.Dropout_dropout                         -  \n",
      "9_encoder.layers.0.LayerNorm_self_attn_layer_norm      256.0  \n",
      "10_encoder.layers.0.positionwise_feedforward.Li...  131.072k  \n",
      "11_encoder.layers.0.positionwise_feedforward.Dr...         -  \n",
      "12_encoder.layers.0.positionwise_feedforward.Li...  131.072k  \n",
      "13_encoder.layers.0.Dropout_dropout                        -  \n",
      "14_encoder.layers.0.LayerNorm_ff_layer_norm            256.0  \n",
      "15_encoder.layers.1.self_attention.Linear_fc_q       65.536k  \n",
      "16_encoder.layers.1.self_attention.Linear_fc_k       65.536k  \n",
      "17_encoder.layers.1.self_attention.Linear_fc_v       65.536k  \n",
      "18_encoder.layers.1.self_attention.Dropout_dropout         -  \n",
      "19_encoder.layers.1.self_attention.Linear_fc_o       65.536k  \n",
      "20_encoder.layers.1.Dropout_dropout                        -  \n",
      "21_encoder.layers.1.LayerNorm_self_attn_layer_norm     256.0  \n",
      "22_encoder.layers.1.positionwise_feedforward.Li...  131.072k  \n",
      "23_encoder.layers.1.positionwise_feedforward.Dr...         -  \n",
      "24_encoder.layers.1.positionwise_feedforward.Li...  131.072k  \n",
      "25_encoder.layers.1.Dropout_dropout                        -  \n",
      "26_encoder.layers.1.LayerNorm_ff_layer_norm            256.0  \n",
      "27_encoder.layers.2.self_attention.Linear_fc_q       65.536k  \n",
      "28_encoder.layers.2.self_attention.Linear_fc_k       65.536k  \n",
      "29_encoder.layers.2.self_attention.Linear_fc_v       65.536k  \n",
      "30_encoder.layers.2.self_attention.Dropout_dropout         -  \n",
      "31_encoder.layers.2.self_attention.Linear_fc_o       65.536k  \n",
      "32_encoder.layers.2.Dropout_dropout                        -  \n",
      "33_encoder.layers.2.LayerNorm_self_attn_layer_norm     256.0  \n",
      "34_encoder.layers.2.positionwise_feedforward.Li...  131.072k  \n",
      "35_encoder.layers.2.positionwise_feedforward.Dr...         -  \n",
      "36_encoder.layers.2.positionwise_feedforward.Li...  131.072k  \n",
      "37_encoder.layers.2.Dropout_dropout                        -  \n",
      "38_encoder.layers.2.LayerNorm_ff_layer_norm            256.0  \n",
      "39_decoder.Embedding_tok_embedding                    9.216k  \n",
      "40_decoder.Embedding_pos_embedding                     25.6k  \n",
      "41_decoder.Dropout_dropout                                 -  \n",
      "42_decoder.layers.0.self_attention.Linear_fc_q       65.536k  \n",
      "43_decoder.layers.0.self_attention.Linear_fc_k       65.536k  \n",
      "44_decoder.layers.0.self_attention.Linear_fc_v       65.536k  \n",
      "45_decoder.layers.0.self_attention.Dropout_dropout         -  \n",
      "46_decoder.layers.0.self_attention.Linear_fc_o       65.536k  \n",
      "47_decoder.layers.0.Dropout_dropout                        -  \n",
      "48_decoder.layers.0.LayerNorm_self_attn_layer_norm     256.0  \n",
      "49_decoder.layers.0.encoder_attention.Linear_fc_q    65.536k  \n",
      "50_decoder.layers.0.encoder_attention.Linear_fc_k    65.536k  \n",
      "51_decoder.layers.0.encoder_attention.Linear_fc_v    65.536k  \n",
      "52_decoder.layers.0.encoder_attention.Dropout_d...         -  \n",
      "53_decoder.layers.0.encoder_attention.Linear_fc_o    65.536k  \n",
      "54_decoder.layers.0.Dropout_dropout                        -  \n",
      "55_decoder.layers.0.LayerNorm_enc_attn_layer_norm      256.0  \n",
      "56_decoder.layers.0.positionwise_feedforward.Li...  131.072k  \n",
      "57_decoder.layers.0.positionwise_feedforward.Dr...         -  \n",
      "58_decoder.layers.0.positionwise_feedforward.Li...  131.072k  \n",
      "59_decoder.layers.0.Dropout_dropout                        -  \n",
      "60_decoder.layers.0.LayerNorm_ff_layer_norm            256.0  \n",
      "61_decoder.layers.1.self_attention.Linear_fc_q       65.536k  \n",
      "62_decoder.layers.1.self_attention.Linear_fc_k       65.536k  \n",
      "63_decoder.layers.1.self_attention.Linear_fc_v       65.536k  \n",
      "64_decoder.layers.1.self_attention.Dropout_dropout         -  \n",
      "65_decoder.layers.1.self_attention.Linear_fc_o       65.536k  \n",
      "66_decoder.layers.1.Dropout_dropout                        -  \n",
      "67_decoder.layers.1.LayerNorm_self_attn_layer_norm     256.0  \n",
      "68_decoder.layers.1.encoder_attention.Linear_fc_q    65.536k  \n",
      "69_decoder.layers.1.encoder_attention.Linear_fc_k    65.536k  \n",
      "70_decoder.layers.1.encoder_attention.Linear_fc_v    65.536k  \n",
      "71_decoder.layers.1.encoder_attention.Dropout_d...         -  \n",
      "72_decoder.layers.1.encoder_attention.Linear_fc_o    65.536k  \n",
      "73_decoder.layers.1.Dropout_dropout                        -  \n",
      "74_decoder.layers.1.LayerNorm_enc_attn_layer_norm      256.0  \n",
      "75_decoder.layers.1.positionwise_feedforward.Li...  131.072k  \n",
      "76_decoder.layers.1.positionwise_feedforward.Dr...         -  \n",
      "77_decoder.layers.1.positionwise_feedforward.Li...  131.072k  \n",
      "78_decoder.layers.1.Dropout_dropout                        -  \n",
      "79_decoder.layers.1.LayerNorm_ff_layer_norm            256.0  \n",
      "80_decoder.layers.2.self_attention.Linear_fc_q       65.536k  \n",
      "81_decoder.layers.2.self_attention.Linear_fc_k       65.536k  \n",
      "82_decoder.layers.2.self_attention.Linear_fc_v       65.536k  \n",
      "83_decoder.layers.2.self_attention.Dropout_dropout         -  \n",
      "84_decoder.layers.2.self_attention.Linear_fc_o       65.536k  \n",
      "85_decoder.layers.2.Dropout_dropout                        -  \n",
      "86_decoder.layers.2.LayerNorm_self_attn_layer_norm     256.0  \n",
      "87_decoder.layers.2.encoder_attention.Linear_fc_q    65.536k  \n",
      "88_decoder.layers.2.encoder_attention.Linear_fc_k    65.536k  \n",
      "89_decoder.layers.2.encoder_attention.Linear_fc_v    65.536k  \n",
      "90_decoder.layers.2.encoder_attention.Dropout_d...         -  \n",
      "91_decoder.layers.2.encoder_attention.Linear_fc_o    65.536k  \n",
      "92_decoder.layers.2.Dropout_dropout                        -  \n",
      "93_decoder.layers.2.LayerNorm_enc_attn_layer_norm      256.0  \n",
      "94_decoder.layers.2.positionwise_feedforward.Li...  131.072k  \n",
      "95_decoder.layers.2.positionwise_feedforward.Dr...         -  \n",
      "96_decoder.layers.2.positionwise_feedforward.Li...  131.072k  \n",
      "97_decoder.layers.2.Dropout_dropout                        -  \n",
      "98_decoder.layers.2.LayerNorm_ff_layer_norm            256.0  \n",
      "99_decoder.Linear_fc_out                              9.216k  \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "                         Totals\n",
      "Total params          4.032548M\n",
      "Trainable params      4.032548M\n",
      "Non-trainable params        0.0\n",
      "Mult-Adds             4.014848M\n",
      "==============================================================================================================\n"
     ]
    }
   ],
   "source": [
    "from torchsummaryX import summary\n",
    "\n",
    "df = summary(model, torch.zeros((1,21)).cuda().long(), torch.zeros((1,21)).cuda().long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "571cc6a11a7c82fafe4044dd42511b8ce244d0e4fc43fb5f49a40a719a7ff563"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
